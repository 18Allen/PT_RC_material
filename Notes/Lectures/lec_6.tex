\chapter{RC 6}
\recitation{6}{17 Nov. 18:30}{}

\begin{remark}
    Errata: page 5 of the "Continuous random variables" handout:\\
            \begin{itemize}
                \item \(\cancelto{\mu \in \mathbf{R} }{\mu \mathbf{R}} \) 
                \item \(X \sim \text{Normal}(\mu, \cancelto{\sigma^2 }{\sigma}) \) 
                \item \(N(\mu , \cancelto{\mu^2 }{\mu})\)
                \item \(\cancelto{\frac{1}{\sqrt{2 \pi \sigma^2 } }}{\frac{1}{\sqrt{2 \pi \sigma } }}\)  
            \end{itemize} 
\end{remark}

\section{Review: Laws of large number}
\[
    \text{Markov inequality} \implies \text{Chebyshev inequality} \implies \text{WLLN}  
\]


\subsection{Converge in probability}
If we say \textbf{ \(X_n \to c\) in probability}, we means the sequence of random varaibles \((X_i)_{i=1}^\infty\) will eventually be really close to \(c\) in a way that
those \(\omega \) that does not map close to \(c\) will eventually has measure zero. 
\[
    \lim\limits_{n \to \infty} P(|X_n -c| \geq \epsilon  ) = 0
\]    
for every \(\epsilon >0\). 
\subsection{The weak law of large number (WLLN)}
To give an intuition of WLLN, you may understand it as an justification of "Taking average of many sample will be really close to expectation value". 
\begin{theorem}[Weak Law of Large Numbers] 
    (Thm. 4 in \cite{Und_Chatterjee})\\
    Let \(X_1,X_2, \dots\) be an \textbf{i.i.d.} sequence of random variables with expected value \(\mu \) and finite variance. For each n, let 
    \[
        \bar{X_n} \coloneqq \frac{1}{n} \sum_{i=1}^{n} X_{i} 
    \]   
    be the average of the first \(n\) of these variables. Then as \(n \to \infty \), \(\bar{X_n} \to  \mu \) \textbf{in probability}.    
\end{theorem}
Direct application of Chebyshev inequality. 

In many cases, you can't expect condition like i.i.d., but as we can see in the following example, you actually don't need it. WLLN only requires \textbf{second moment information}, such as covariance and variance.  
\begin{eg}[You Don't need i.i.d. for WLLN]
    (\cite{IntroPanchenko} Exercise 2.2.1) \\ 
    Suppose that random variables \(X_1, \dots, X_n\) are uncorrelated, \(\mu  = E[\bar{X_n}]\) and Var(\(X_{i} \)) \(\leq \sigma^2\) for all \(i \leq n\). 
    Then show that WLLN still holds.     
\end{eg}
Notice that the key is to show that Var(\(X_n\)) grows with the rate \(\theta(n^2)\) as \(n \to \infty \). 
\\
\\
Combine with the use of indicator function, we can study random graph. 
\begin{eg}[Expected number of triangles on Erdős-Rényi random graph]
    (Adapted from Prof. Chatterjee's STAT310 note Exercise 8.4.5)
    Define an undirected random graph on n vertices by putting an edge between any two vertices with probability \(p\)  and excluding the edge with probability \(1-p\) , all edges independent.
    This is known as the Erdős-Rényi \(G(n,p)\) random graph. 
    \begin{itemize}
        \item If \(T_{n,p}\) is the number of triangles in this random graph, use the method of indicator to represent \(T_{n,p}\) . 
        \item Before we made calculation, can you guess why \(T_{n,p} / n^3 \to p^3 /6\) ? \\
            Hint: Make reasons out of the number of indicator function, probability of one triangles being connected. 
        \item Show that \(T_{n,p} / n^3 \to p^3 / 6 \) in probability with WLLN. 
            Hint: Break down the \(\sum \text{Cov} \) to cases depend on the relation of two triangles (share one edges, no share edges, same triangles)
    \end{itemize} 
\end{eg}

\section{Continuous Random Variables}
\subsection{Some Continuous Random Variables}
These are some continuous r.v. that you should keep in mind.  
\begin{itemize}
    \item \textbf{Uniform r.v. \(X \sim \text{Unif}([a,b])\) }:\\
        \[
            f(x) = \begin{cases}
                \frac{1}{b-a}, &\text{ if } x \in [a,b] ;\\
                0, &\text{ otherwise }  .
            \end{cases}
        \]
    \item \textbf{Exponential r.v. \(X \sim \text{Exp}(\lambda )  \)}: \\
    \[
        f(x) = \begin{cases}
            \lambda e^{-\lambda x}, &\text{ if } x\geq 0 ;\\
            0 , &\text{ otherwise }  .
        \end{cases}
    \]
    \item \textbf{Normal(Gaussian) r.v. \(X \sim N(\mu ,\sigma^2) \) }: \\   
    \[
        f(x) = \frac{1}{\sqrt{2 \pi \sigma^2 }} e^{\frac{-(x-\mu )^2}{2 \sigma^2}}
    \]
    The \(N(0,1)\) distribution is called \textbf{standard normal distribution}.   
\end{itemize}

Before we introduce a more formal definition concerning continuous random variables, let's try to extend our understanding from discrete random ones. 
\begin{eg}
    (\cite{IntroPanchenko} Exercise 4.1.2.) Compute the expectation of a uniform and
exponential random variable.
\end{eg}
We basically just turn summation to integration. 

\begin{exercise}
    (\cite{IntroPanchenko} Exercise 4.1.4.) Compute the expectation of \(E |X|\), where \(X\) follows \(N(0,\sigma^2)\).  
\end{exercise}
\subsection{Probability Density Function}
We may ignore some intricate definition of "niceness" of p.d.f. for now. Just remember that if a \textbf{non-negative} function \(f\) is given as p.d.f of random variable \(X\), 
the probability of \(X\) taking value in \(A\) is 
\[
    P(X \in A) = P(A) = \int_A f(x)dx
\] 

This means that we care about the value \textbf{"after" integration}, not density per se. .
\begin{itemize}
    \item \textbf{p.d.f. can take on value larger than 1} \\ 
            For example: \(f(x) = 2 1_{|X| \leq  \frac{1}{2}} \). \(f(0) = 2\) but \(\int_{-1 /2 }^{1 /2} f(x)dx  = 1\).   
    \item \textbf{P(\(\mathbb{R} \) )}\\ 
            By the definition above, \(P(\mathbb{R} ) = \int_{-\infty }^{\infty} f(x)dx = 1 \)   
\end{itemize}

\subsection{Expectation and Variance}. 
The expectation of a continuous random variable \(X\) with p.d.f. \(f\) is defined as 
\[
    E[X] = \int_{-\infty }^{\infty } x f(x)dx
\] 
Providing that \(xf(x)\) is absolutely integrable. (The reason is that we will sometimes change the order of integration). 

Given the condition that  \(x^2 f(x)\) is absolutely integrable, we may define the \textbf{variance} of \(X\) as 
\[
    \text{Var}(X) = E[X^{2} ] - E[X]^2 
\]

\subsection{Cumulative Distribution Function}
Recall the first problem in quiz two that we ask you to draw a function \(F_X(x) = P(X \leq x)\). The cumulative distribution function (c.d.f.) of a continuous random variables \(X\) is defined just like the one for discrete r.v. . 
\[
    F_X(x) =F(x) =  P(X \leq  x) = \int_{-\infty }^x f(t)dt
\] 
In normal cases where your p.d.f. is continuous, the fundamental theorem of calculus implies that 
\[
    f(x) = F^\prime (x)
\]
This property can be helpful when you use c.d.f. to compute p.d.f. after transformation. Check out the example in \cite{Und_Chatterjee} page 55 about \(Y \sim X^2\).  

\subsection{Change of Variable Formula}
Let \(X\) be a continuous r.v. and \(u:\mathbb{R}\to \mathbb{R}\) be a \textbf{strictly increasing function}. We define a new r.v. \(Y = u(X)\). 
\\What is the p.d.f. of \(Y\)?  

\[
    P(Y \leq y) = P(u(X) \leq  y) = P(X \leq u^{-1}(y)).
\] 
Using the c.d.f. of \(X\) at \(u^{-1}(y)\) , we can compute the p.d.f. of \(y\) by taking the derivative 
\[
    \frac{d}{dy}F(u^{-1}(y)) = F^\prime (u^{-1}(y))(u^{-1})^\prime (y)
\] 
\begin{remark}
    The case for \textbf{strictly decreasing} \(u\) is to compute the derivative of 
    \[
        P(u(X) \leq y) = P(X \mathbf{\geq } u^{-1}(y)) = 1-F(u^{-1}(y))
    \]  
    Hence, there will be an extra "-" in \(- f(u^{-1}(y)) u^{-1}(y)\).   
\end{remark}

This idea of changing the measure will extend to the change-of-variable for multivariable case. That will be the topic of next week. 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{./Figures/Jacobian.png}
    \caption{\href{https://makeameme.org/meme/when-youre-having-592b93}{source}}
    \label{fig:Jacobian-png}
\end{figure}
\begin{eg}
    (\cite{IntroPanchenko} Exercise 4.1.3.) What is the density of the random variable \(X = g^3\), where \(g\) follows \(N(0,1)\).
\end{eg}
\section{Extra}
\subsection*{"Derive" Gaussian distribution}
Have you ever wonder why Gaussian distribution look like that? \\
\href{https://youtu.be/ebewBjZmZTw}{Normal distribution's probability density function derived in 5min}

\subsection*{"Real" issues}
\begin{itemize}
    \item Not every set in \(\mathbb{R}\) has probability defined. 
    \item If we change value of a p.d.f on a measure-zero set, the result will still be a p.d.f.    
\end{itemize}
