\chapter{RC 4}
%\lecture{3}{22 Sep. 18:30}{Temp}
\recitation{4}{27 Oct. 18:30}{}


\subsection{Variance}
Variance of a random variable \(X\) is the second moment of the demeaned \(X-E[X]\), i.e
\[
    \text{Var} (X) = E[(X-E[X])^2] = E[X^2] - E[X]^2
\]
\subsection{Covariance}
The definition of covariance is like that for variance, but for two random variable. 
\[
    \text{Cov} (X,Y) = E[(X-E[X])(Y-E[Y])]
\]
Take \(Y\) and \(X\) the same then you get the definition for variance. 
Below are two properties for the use of covariance
\begin{itemize}
    \item (Proposition 8)Bilinear:\\
    Let \(X_1,\dots, X_m\), \(Y_1, \dots, Y_n\) be random variables and \(a_1,\dots,a_m,  b_1,\dots,b_n\) be real numbers. Let \(\mathcal{U} = a_1 X_1 + \dots, a_m X_m \) and \(V = b_1 Y_1, \dots, b_n Y_n\). Then
    \[
        \text{Cov} (\mathcal{\MakeUppercase{U}} ,V) = \sum_{i=1}^{m} \sum_{j=1}^{n} a_i b_j \text{Cov}(X_{i}, Y_{j}). 
    \]    
    
    The immediate use of this proposition is Corollary 4 and 6. This is useful when you consider the variance of something like \(S_n = \sum_{i=1}^{n} X_i\), where \(X_i\)'s are i.i.d random variable. Then, \(\text{Var} (S_n) = n \cdot \text{Var}(X_1) \). You can verify this by thinking the variance of Bernoulli(p) and Bin(n,p), one is \(p(1-p)\) and the other is \(np(1-p)\).     
   
        
    \item (Corrollary 5)Independent implies covariance zero\\
    If two random variables are independent, then there covariance is zero (\textbf{NOT the other way around!})
    The exercise below show that the converse is false.  
\end{itemize}
\begin{exercise}
    Find a counterexample of two discrete random variable \(X,Y\) such that \(\text{Cov}(X,Y) = 0  \), but they are not independent. 
\end{exercise}
Answer: \footnote[1]{\(Y\) with \(f_y(1) = f_y(-1) = f(0) = \frac{1}{3}\) (uniform), and \(X = |Y|\) } 
\section{The method of indication}
(\cite{Und_Chatterjee} p.33)" The method of indicators is a technique for evaluating the expected value/variance of a random variable by finding a way to write it as a sum of indicator function."

The \textbf{indicator} of \(A\) is a random variable, denoted by \(1_A\), (you can understand it as taking \(A\) or not), defined as follows  
\[
    1_A(\omega) = \begin{cases} 1, \quad \text{if } \omega \in A \\ 0,\quad \text{if } \omega \notin A \end{cases}
\]

If \(X\) can be represented as \(X = \sum_{i=1}^{n} 1_{A_i}\), we can use \textbf{linearity of expectation} to write 
\[
    E[X] = \sum_{i=1}^{n} E[1_{A_i}]
\]  
(Remember here we don't have to worry about independence of \(1_{A_i}\) .)
\subsection{For expectation}
\begin{eg}[Coin Run ].\\
   \begin{itemize}
       \item A biased coin is tossed \(n\)  times, and heads shows with probability p on each toss. A run is a
sequence of throws which result in the same outcome, so that, for example, the sequence 
\[
HHTHTTH
\]    
contains five runs. Show that the expected number of runs is \(1 + 2(n - l)p(1 - p) \). Find the variance
of the number of runs. 
        \item As a review, what is the expected length of a run. 
        \item A head run is simply a continuous sequence of heads. Consider the first problem but with head run.
   \end{itemize} 
\end{eg}
\begin{exercise}
    Of the \(2n\)  people in a given collection of \(n\)  couples, exactly \(m\)  die. Assuming that the m have
been picked at random, find the mean number of surviving couples. This problem was formulated by
Daniel Bernoulli in 1768.\\
\end{exercise}
Hint: Find indicator on the survival for each couple.
%\subsection{For variance}
\begin{figure*}[hbt]
    \centering
    \includegraphics[width=0.25\textwidth]{./Figures/Keep_Calm_and_Carry_On.png}
    \caption*{\href{https://commons.wikimedia.org/wiki/File:Keep_Calm_and_Carry_On_Poster.svg}{Wiki} Best of luck with your exams.}
\end{figure*}