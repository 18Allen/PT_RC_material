\chapter{RC 8}
\recitation{8}{1 Dec. 18:30}{}

\subsection*{Application of }


\section*{Things that you can migrate from discrete to cont.}
For 1D cont. r.v., apart from the definition of expectation, variance, and covariance, any other things depend on them still work, including
\begin{itemize}
    \item Expectation of some function defined on random variables. 
    \item Variance of a sum of r.v. is the sum of covariances. 
    \item Inequalities(Markov's and Chebyshev... )
    \item Weak law of large numbers.
\end{itemize}
 

\section*{Mean vector and covariance matrix}
(\cite*{Und_Chatterjee})
Let \(X = (X_1, \dots, X_n)\) be an n-dimensional random vector (discrete or continuous). 
\begin{itemize}
    \item \textbf{Mean vector} \(E[X]\)\\
    The \textbf{mean vector} of  \(X\), denoted by \(E[X]\) is the  
    \item \textbf{Covariance matrix} Cov(\(X\))\\
    The \textbf{covariance matrix} of \(X\), denoted by Cov(\(X\)) is the nxn matrix \(\Sigma\) with 
    \[
        (\Sigma )_{ij} = \sigma_{ij} = \text{Cov} (X_i,X_j)
    \]   
    \begin{eg}
        \(X_1, \dots, X_n\) are i.i.d. standard normal r.v.. Define a random vector \(X = (X_1, \dots,X_n)\) (standard normal vector). \\
        Then, \(\text{Cov} (X) = I\)  
    \end{eg}
    \begin{remark}
        The covaraince matrix \(\Sigma \) has some properies  
        \begin{itemize}
            \item Symmetric
            \[
                (\Sigma )_{ij} = \text{Cov}(X_{i} ,X_{j} ) = \text{Cov} (X_{j} ,X_{i}) = (\Sigma)_{ji} 
            \]
            Therefore, \(\Sigma \) is a \textbf{diagonalizable matrix} 
            \item positive semi definite
                In other word, given any \(u \in \mathbb{R}^n\) 
                \[
                    u^T \Sigma u \geq 0
                \]
    

        \end{itemize}
    \end{remark}
    \item Linear transformation \(Y = AX\)\\ 
    \(E[Y]\) 
    \[
        E[Y]  = E[AX] = AE[X]
    \]
    \(\Sigma \) 
    \[
        \sigma_{ij} =  \text{Cov} ((AX)_i, (AX)_{j} ) = \sum_{1\leq p,q \leq n}^{n} a_{ip}a_{jq} \text{Cov}(X_{p} ,X_{q} ) 
    \]
    Rearrange it you get 
    \[
        \Sigma_Y = A \Sigma_X A^T
    \] 
\end{itemize}
One useful fact to remember is 
\begin{theorem}
    Given a standard normal random vector \(X\), and let \(Z = \mu  + AX\), where \(\mu \in \mathbb{R}^n\) and \(A\) is invertible. 
    \\Then, the joint p.d.f. of \(Z\) is 
    \[
        h(z) = \frac{1}{(2\pi)^{\frac{n}{2}} (\text{det}(\Sigma) )^{\frac{1}{2}}} e^{-\frac{1}{2} (z-\mu )^T \Sigma ^{-1} (z-\mu ) }
    \]     
    where \(\Sigma  = A A^T\) is \(\Sigma_{AX} \). 
\end{theorem}
To give a simplified idea on page 74,75 of \cite*{Und_Chatterjee} and proposition 13. 
\begin{eg}[diagonalization]
    Although it is impractical to compute diagonalization of a matrix by hand, but it give you some idea. 
    \\If you are given a mulivariate normal distribution \(Z \sim N(0, \Sigma )\), where 
    \[
        \Sigma  = \begin{bmatrix}
            3 &  -1 \\
            -1 &  3 \\
        \end{bmatrix}
    \]
    Find a matrix \(A\) such that each component of \(AZ\) is independent random variables.   
\end{eg}
\textbf{Sol.}
We have 
    \[
        \Sigma \begin{bmatrix}
             1 \\
             1 \\
        \end{bmatrix} = 2 \begin{bmatrix}
             1 \\
             1 \\
        \end{bmatrix}
    \]
    and 
    \[
        \Sigma \begin{bmatrix}
             1 \\
             -1 \\
        \end{bmatrix} = 4 \begin{bmatrix}
             1 \\
             -1 \\
        \end{bmatrix}
    \]
    Let \(v_1 = \frac{1}{\sqrt{2} }\begin{bmatrix}
        1 &  1 \\
    \end{bmatrix}^T, v_2 = \frac{1}{\sqrt{2} } \begin{bmatrix}
        1 &  -1 \\
    \end{bmatrix}^{T} \), we define \(V = [v_1, v_2]\)
    \\
    Then,
    \[
        \Sigma = V \begin{bmatrix}
            2 &  0 \\
            0 &  4 \\
        \end{bmatrix} V^T 
    \] 
    By checking the form of joint p.d.f. of \(Z\), we have  
 \begin{equation}
    \begin{aligned}
    h(z) &= \frac{1}{(2\pi) (\text{det}(\Sigma) )^{\frac{1}{2}}} e^{-\frac{1}{2} (z)^T \Sigma ^{-1} (z) }
     \\ &= \frac{1}{(2\pi) (2 \cdot 4 )^{\frac{1}{2}}} e^{-\frac{1}{2} (z)^T \begin{bmatrix}
        \frac{1}{2} & 0  \\
        0 & \frac{1}{4}  \\
    \end{bmatrix} V^T (z) }
    \end{aligned}
 \end{equation}
 Hence, the new gaussian normal vector \(Y = V^T Z\) will have diagonal covaraince matrix, i.e. diagonal. 
   
\begin{exercise}
    HW9-6
\end{exercise}


\subsection*{Gaussian Stability}
Remember in RC2 we introduced the stability of Poission random variables.  Here we will show that
\begin{theorem}[Gaussian stability]

If \(a = (a_1, \dots, a_n) \in \mathbb{R}^n\), and \(g\) is a n-dim standard normal vector, then the random variable 
\[
    X = \sum_{i=1}^{n} a_i g_i
\]
has the Gaussian distribution N(0, \(\sum_{i=1}^{n}a_i^2 \) )
\end{theorem} 
\begin{proof}
    This can be shown by considering the sum of two and then using induction. (\cite{Und_Chatterjee} page 63, proposition 11).\\ 
    Here we give a proof with more of linear algebra fasion. (\cite*{IntroPanchenko} Theorem 4.1)
    \\ Main idea:
    \begin{itemize}
        \item Find the complement space to \(q_1 = \frac{a}{\mid a\mid }\), and thus the transformation matrix \(Q\) is unitary.  
        \item For \(Y = QX\), calculate the marginal p.d.f. of \(y_1\). It will be easy because \(y_i\) are independent.   
    \end{itemize}
\end{proof}
Since \(a_{i}g_i \sim N(0,a_i^2) \), this means that the sum of independent
Gaussian random variables is also Gaussian with the
variance equal to the sum of variances.

\begin{exercise}
    HW9-5
\end{exercise}
Hint: HW9-4