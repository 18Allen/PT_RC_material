\documentclass[12pt]{article}

\usepackage[english]{babel}

\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
%\usepackage[legalpaper, landscape, margin=1in]{geometry}
\usepackage{geometry}
\geometry{
a4paper,
left=25mm,
right=25mm,
top=15mm,
}
\usepackage{setspace}
\setstretch{1.25}

\begin{document}

\section*{Erreta}
\subsection*{Q1}
(c) ... size of the randomly chosen group. Let E[\color{red}Y\color{black}] = \(\mu\) and ... 


\section*{Partial solution to HW 6}

\subsection*{Q1}
Checkout out chapter 5 Q5 solution in Janko Graver's book. 
\subsection*{Q2}
(Panchenko Exercise 1.5.2)
There could be many, my solution use shifted normalized Poisson 
Set discrete random variable \(X\). We define the pmf \(f_X\) to be \\
\[ 
    f_{X}(x) 
    \begin{cases}
      1-e^{-10}  , &\text{ if } x = 0 ;\\
      e^{-10}e^\lambda \frac{\lambda^{(x-1)}}{(x-1)!}  , &\text{ if } x \in \mathbb{N} .
    \end{cases}
\]
Then, \(P(X > 0) = e^{-10}\) is easy to check. Now we have to set the correct \(\lambda \). 
By some calculation you can see that 
\begin{equation*}
    \begin{aligned}
    E[X] &= \sum_{x=1}^{\infty}((x-1) + 1) e^{-10}e^\lambda \frac{\lambda^{(x-1)}}{(x-1)!} 
        \\ &= e^{-10}\underbrace{\sum_{x=0}^{\infty}x e^\lambda \frac{\lambda^{x}}{(x)!}}_{\text{expectation of Poisson}(\lambda ) } + e^{-10}
        \\ &= e^{-10}(\lambda +1)
    \end{aligned}
\end{equation*}
Hence, set \(\lambda  = e^{20} -1\) we will have \(E[X] = e^{10} \).   

\hspace{\textwidth}\(\square\) 
\subsection*{Q3}
This is the same as the exercise in RC4 exercise for the indicator method. 
The number of pairs of animals alive, \(N\), is not an easy random variable to compute. Therefore, we may consider "when will \(N\) increase?". 
This is easier, \(N\) increase 1 if a pair of animals is alive.
Hence, the indicator we consider is \(1_{A_i}, i = 1,\dots, n\), where \(A_i\) means the event that the ith pair of animal is alive. 

Then, we have  \(E[1_{A_i}] = P(A_i) = \begin{pmatrix}
     2(n-1) \\
     m \\
\end{pmatrix} / \begin{pmatrix}
     2n \\
     m \\
\end{pmatrix}\)

Consequently, by linearity of expectation,\\
\(E[N] = \sum_{i=1}^{n}E[1_{A_i}] = nP(A_1) = \frac{(2n-m)(2n-m-1)}{2(2n-1)}\) pairs.    

\hspace{\textwidth}\(\square\) 
\subsection*{Q4}
Again, this is a problem using the indicator method. 

\subsection*{Q5}
\subsection*{Q6}
\begin{itemize}
    \item[(a)] Calculate \(E[X^2] = p\), and use \(\text{Var}(X) = E[X^2] - E[X]^2\).   
    \item[(b)] We did that in recitation class. If you forget, try to use the method introduced in handout Chapter 3 page 14 of from prof. Sheu.  
    \item[(c)] Recall that \(E[aX +b] = aE[X] + b\) due to the linearity of expectation. We have \(Var(aX+b) = E[(aX+b - (aE[X]+b))^2] = E[a^2(X-E[X])^2] = a^2 Var(X) \) 
\end{itemize}
\subsection*{Q7}
\subsection*{Q8}
\subsection*{Q9}
\subsection*{Q10}
\subsection*{Q11}
Use the method of indicators, for $i\neq j$, we can write 
\begin{equation*}
\begin{aligned}
\mathbb{E}(X_{e(i)}X_{e(j)}) &= \mathbb{E}(X_{e(i)}X_{e(j)}\mathds{1}_{\{e(i)\neq e(j)\}}+X_{e(i)}X_{e(j)}\mathds{1}_{\{e(i)= e(j)\}}) \\&
=\mathbb{E}(X_{e(i)}X_{e(j)}\mathds{1}_{\{e(i)\neq e(j)\}})+\mathbb{E}(X_{e(i)}X_{e(j)}\mathds{1}_{\{e(i)= e(j)\}}).
\end{aligned}
\end{equation*}
Adapt the indicator method again, you can calculate
\begin{equation*}
\begin{aligned}
\mathbb{E}(X_{e(i)}X_{e(j)}\mathds{1}_{\{e(i)\neq e(j)\}}) &= \sum_{k\neq i;\;l\neq j;\; k\neq l}\mathbb{E}(X_{e(i)}X_{e(j)}\mathds{1}_{\{e(i)=k,\,e(j)=l\}}) \\&
=\sum_{k\neq i;\;l\neq j;\; k\neq l}\mathbb{E}(X_k X_l\mathds{1}_{\{e(i)=k,\,e(j)=l\}}) \\&
=\sum_{k\neq i;\;l\neq j;\; k\neq l}\mathbb{E}(X_k)\mathbb{E}(X_l)\mathbb{E}(\mathds{1}_{\{e(i)=k\}})\mathbb{E}(\mathds{1}_{\{e(j)=l\}}) \\&
=0,
\end{aligned}
\end{equation*}
and
\begin{equation*}
\begin{aligned}
\mathbb{E}(X_{e(i)}X_{e(j)}\mathds{1}_{\{e(i)=e(j)\}}) &= \sum_{k\neq i;\;k\neq j}\mathbb{E}(X_{e(i)}X_{e(j)}\mathds{1}_{\{e(i)=k,\,e(j)=k\}}) \\&
=\sum_{k\neq i;\;k\neq j}\mathbb{E}(X_k^2\mathds{1}_{\{e(i)=k,\,e(j)=k\}}) \\&
=\sum_{k\neq i;\;k\neq j}\mathbb{E}(X_k^2)\mathbb{E}(\mathds{1}_{\{e(i)=k\}})\mathbb{E}(\mathds{1}_{\{e(j)=k\}}) \\&
=\sum_{k\neq i;\;k\neq j}1\cdot\mathbb{P}(e(i)=k)\mathbb{P}(e(j)=k) \\&
=\sum_{k\neq i;\;k\neq j}\frac{1}{(n-1)^2} = \frac{n-2}{(n-1)^2}.
\end{aligned}
\end{equation*}
Hence $\mathbb{E}(X_{e(i)}X_{e(j)})=\frac{n-2}{(n-1)^2}$ for $i\neq j$.

When $i=j$, $\mathbb{E}\big(X_{e(i)}^2\big)=\sum_{k\neq i}\mathbb{E}(X_{e(i)}^2\mathds{1}_{\{e(i)=k\}})=\sum_{k\neq i}\mathbb{E}(X_k^2\mathds{1}_{\{e(i)=k\}})$. Use the independence calculation again, you can see that $\mathbb{E}\big(X_{e(i)}^2\big)=(n-1)\cdot1\cdot\frac{1}{(n-1)}=1$.

Then the variance can be computed as
\begin{equation*}
\begin{aligned}
Var(X_{e(1)}+\cdots+X_{e(n)}) &= \sum_{i,j}Cov(X_{e(i)}, X_{e(j)})=\sum_{i,j}\mathbb{E}(X_{e(i)}X_{e(j)})-\mathbb{E}(X_{e(i)})\mathbb{E}(X_{e(j)}) \\&
=\sum_{i,j}\mathbb{E}(X_{e(i)}X_{e(j)})
=\sum_{i=j}1+\sum_{i\neq j}\frac{n-2}{(n-1)^2}=n+\frac{n(n-2)}{n-1}.
\end{aligned}
\end{equation*}
You can check that $\mathbb{E}(X_{e(i)})=0$ with the similar method.
$\hspace{\fill}\square$


\end{document}